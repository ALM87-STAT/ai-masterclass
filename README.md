# Masterclass en Inteligencia Artificial
Repositorio de la Masterclass en Inteligencia Artificial


## Bienvenido

Bienvenido al repositorio de datos para el curso **Masterclass en Inteligencia Artificial** de Kirill Eremenko, Hadelin de Ponteves y Juan Gabriel Gomila. Aquí encontrarás los datasets y materiales complementarios del curso. Disfrútalos!

### Section 1. Introduction

* [Estructura del curso](Recursos Adicionales/Estructura del Curso.png)
* [Modelo del Mundo Completo](Recursos Adicionales/World Model.pdf)

#### Lecturas Adicionales

* [Artículo del Modelo del Mundo Completo](https://worldmodels.github.io)
* [El mejor recurso sobre Estrategias Evolutivas](https://blog.otoro.net/2017/10/29/visual-evolution-strategies/)

### Sección 2. Redes Neuronales Artificiales (ANN)

#### Lecturas Adicionales

* Yann LeCun et al., 1998, [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)
* By Xavier Glorot et al., 2011, [Deep sparse rectifier neural networks](http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf)
* CrossValidated, 2015, [A list of cost functions used in neural networks, alongside applications](http://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)
* Andrew Trask, 2015, [A Neural Network in 13 lines of Python (Part 2 - Gradient Descent)](https://iamtrask.github.io/2015/07/27/python-network-part2/)
* Michael Nielsen, 2015, [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap2.html)

### Sección 3. Redes Neuronales Convolucionales (CNN)

#### Lecturas Adicionales

* Yann LeCun et al., 1998, [Gradient-Based Learning Applied to Document Recognition](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)
* Jianxin Wu, 2017, [Introduction to Convolutional Neural Networks](http://cs.nju.edu.cn/wujx/paper/CNN.pdf)
* C.-C. Jay Kuo, 2016, [Understanding Convolutional Neural Networks with A Mathematical Model](https://arxiv.org/pdf/1609.04112.pdf)
* Kaiming He et al., 2015, [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852.pdf)
* Dominik Scherer et al., 2010, [Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition](http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf)
* Adit Deshpande, 2016, [The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)
* Rob DiPietro, 2016, [A Friendly Introduction to Cross-Entropy Loss](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)
* Peter Roelants, 2016, [How to implement a neural network Intermezzo 2](http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/)

### Section 4. AutoEncoder (AE)

#### Lecturas Adicionales
Malte Skarupke, 2016, Neural Networks Are Impressively Good At Compression
Francois Chollet, 2016, Building Autoencoders in Keras
Chris McCormick, 2014, Deep Learning Tutorial - Sparse Autoencoder
Eric Wilkinson, 2014, Deep Learning: Sparse Autoencoders
Alireza Makhzani, 2014, k-Sparse Autoencoders
Pascal Vincent, 2008, Extracting and Composing Robust Features with Denoising Autoencoders
Salah Rifai, 2011, Contractive Auto-Encoders: Explicit Invariance During Feature Extraction
Pascal Vincent, 2010, Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion
Geoffrey Hinton, 2006, Reducing the Dimensionality of Data with Neural Networks

### Section 5. Variational AutoEncoder (VAE)

#### Lecturas Adicionales

* [Irhum Shafkat, 2018, Intuitively Understanding Variational Autoencoders](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)
* [Diederik P. Kingma and Max Welling, 2014, Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114.pdf)
Francois Chollet, 2016, Building Autoencoders in Keras
Chris McCormick, 2014, Deep Learning Tutorial - Sparse Autoencoder
Eric Wilkinson, 2014, Deep Learning: Sparse Autoencoders
Alireza Makhzani et al., 2014, k-Sparse Autoencoders
Pascal Vincent et al., 2008, Extracting and Composing Robust Features with Denoising Autoencoders
Salah Rifai et al., 2011, Contractive Auto-Encoders: Explicit Invariance During Feature Extraction
Pascal Vincent et al., 2010, Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion
Geoffrey Hinton et al., 2006, Reducing the Dimensionality of Data with Neural Networks

### Section 6. Implementing the CNN-VAE

### Section 7. Recurrent Neural Networks (RNN)

#### Lecturas Adicionales
Oscar Sharp & Benjamin, 2016, Sunspring
Sepp (Josef) Hochreiter, 1991, Untersuchungen zu dynamischen neuronalen Netzen
Yoshua Bengio, 1994, Learning Long-Term Dependencies with Gradient Descent is Difficult
Razvan Pascanu, 2013, On the difficulty of training recurrent neural networks
Sepp Hochreiter & Jurgen Schmidhuber, 1997, Long Short-Term Memory
Christopher Olah, 2015, Understanding LSTM Networks
Shi Yan, 2016, Understanding LSTM and its diagrams
Andrej Karpathy, 2015, The Unreasonable Effectiveness of Recurrent Neural Networks
Andrej Karpathy, 2015, Visualizing and Understanding Recurrent Networks
Klaus Greff, 2015, LSTM: A Search Space Odyssey
Xavier Glorot, 2011, Deep sparse rectifier neural networks

### Section 7. Mixture Density Network (MDN)

### Section 8. Implementing the MDN-RNN

### Section 9. Reinforcement Learning

#### Lecturas Adicionales
Arthur Juliani, 2016, Simple Reinforcement Learning with Tensorflow (10 Parts)
Richard Sutton et al., 1998, Reinforcement Learning I: Introduction
Richard Bellman, 1954, The Theory of Dynamic Programming
D. J. White, 1993, A Survey of Applications of Markov Decision Processes
Martijn van Otterlo, 2009, Markov Decision Processes: Concepts and Algorithms
Richard Sutton, 1988, Learning to Predict by the Methods of Temporal Differences

### Section 10. Deep Neuro Evolution (GA, ES)
